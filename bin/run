#!/usr/bin/env bash

set -eo pipefail
set -x

# default variables
: "${PORT:=8000}"
: "${SLEEP:=1}"
: "${TRIES:=60}"

usage() {
  echo "usage: bin/run flower|web|worker|scheduler"
  exit 1
}

wait_for() {
  tries=0
  echo "Waiting for $1 to listen on $2..."
  while true; do
    [[ $tries -lt $TRIES ]] || return
    (echo > /dev/tcp/$1/$2) >/dev/null 2>&1
    result=
    [[ $? -eq 0 ]] && return
    sleep $SLEEP
    tries=$((tries + 1))
  done
}

gcp_default_extras() {
python - <<END
import json

extra = {
    "extra__google_cloud_platform__project": "dummy_project_id",
    "extra__google_cloud_platform__keyfile_dict": json.dumps({})
}

print(json.dumps(extra))
END
}

aws_default_extras() {
python - <<END
import json

extra = {
    "aws_access_key_id": "dummy_access_key_id",
    "aws_secret_access_key": "dummy_secret_access_key",
}

print(json.dumps(extra))
END
}

init_connections() {
    # set dummy credentials
    export AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-dummy_access_key_id}
    export AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-dummy_secret_access_key}

    airflow connections --delete --conn_id databricks_default

    airflow connections --add \
        --conn_id databricks_default \
        --conn_type databricks \
        --conn_host https://dbc-caf9527b-e073.cloud.databricks.com \
        --conn_extra "{\"token\":\"${DB_TOKEN}\"}"

    gcp_conn=(
      "google_cloud_airflow_dataproc"
      "google_cloud_derived_datasets"
      "google_cloud_prio_a"
      "google_cloud_prio_admin"
      "google_cloud_prio_b"
    )
    for conn_id in "${gcp_conn[@]}"; do
        airflow connections --delete --conn_id "${conn_id}"
        airflow connections --add \
        --conn_id "${conn_id}" \
        --conn_type google_cloud_platform \
        --conn_extra "$(gcp_default_extras)"
    done

    aws_conn=(
      "airflow_taar_rw_s3"
      "amplitude_s3_conn"
      "aws_dev_iam_s3"
      "aws_dev_socorro_telemetry_parquet_s3"
      "aws_dev_telemetry_public_analysis_2_rw"
      "aws_prod_fx_usage_report"
      "aws_prod_probe_scraper"
      "aws_socorro_readonly_s3"
    )
    for conn_id in "${aws_conn[@]}"; do
        airflow connections --delete --conn_id "${conn_id}"
        airflow connections --add \
        --conn_id "${conn_id}" \
        --conn_type s3 \
        --conn_extra "$(aws_default_extras)"
    done
}

init_variables() {
    airflow variables -s \
        "LEANPLUM_FENNEC_NIGHTLY_APP_ID" "fennec-nightly-app-id"

    airflow variables -s \
        "LEANPLUM_FENNEC_NIGHTLY_CLIENT_KEY" "fennec-nightly-client-key"
}

[ $# -lt 1 ] && usage

# Only wait for backend services in development
# http://stackoverflow.com/a/13864829
[ ! -z ${DEVELOPMENT+check} ] && wait_for db 3306 && wait_for redis 6379

case $1 in
  flower)
    exec airflow flower
    ;;
  web)
    airflow initdb
    airflow upgradedb

    # Only init connections in dev
    [ ! -z ${DEVELOPMENT+check} ] && init_connections && init_variables

    exec airflow webserver -p ${PORT} --workers 4
    ;;
  worker)
    exec airflow worker
    ;;
  scheduler)
    exec airflow scheduler
    ;;
  *)
    exec "$@"
    ;;
esac
